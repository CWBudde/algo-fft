//go:build amd64 && asm && !purego

// ===========================================================================
// SSE2 Size-16 Radix-2 FFT Kernels for AMD64 (complex128)
// ===========================================================================

#include "textflag.h"

// Forward transform, size 16, complex128, radix-2
TEXT ·ForwardSSE2Size16Radix2Complex128Asm(SB), NOSPLIT, $0-97
	// Load parameters
	MOVQ dst+0(FP), R8
	MOVQ R8, R14
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ src+32(FP), R13

	CMPQ R13, $16
	JNE  size16_sse2_128_fwd_err

	CMPQ R8, R9
	JNE  size16_sse2_128_fwd_use_dst
	MOVQ R11, R8

size16_sse2_128_fwd_use_dst:
	// Stage 1 & 2 (Combined, fully unrolled with inlined bit-reversal)
	// Bit-reversal pattern: 0,8,4,12,2,10,6,14,1,9,5,13,3,11,7,15
	MOVQ R8, SI
	MOVUPS ·maskNegHiPD(SB), X15

	// Block 0: src[0,8,4,12]
	MOVUPD 0(R9), X0; MOVUPD 128(R9), X1; MOVUPD 64(R9), X2; MOVUPD 192(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 0(SI); MOVUPD X8, 16(SI); MOVUPD X10, 32(SI); MOVUPD X12, 48(SI)

	// Block 1: src[2,10,6,14]
	MOVUPD 32(R9), X0; MOVUPD 160(R9), X1; MOVUPD 96(R9), X2; MOVUPD 224(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 64(SI); MOVUPD X8, 80(SI); MOVUPD X10, 96(SI); MOVUPD X12, 112(SI)

	// Block 2: src[1,9,5,13]
	MOVUPD 16(R9), X0; MOVUPD 144(R9), X1; MOVUPD 80(R9), X2; MOVUPD 208(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 128(SI); MOVUPD X8, 144(SI); MOVUPD X10, 160(SI); MOVUPD X12, 176(SI)

	// Block 3: src[3,11,7,15]
	MOVUPD 48(R9), X0; MOVUPD 176(R9), X1; MOVUPD 112(R9), X2; MOVUPD 240(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 192(SI); MOVUPD X8, 208(SI); MOVUPD X10, 224(SI); MOVUPD X12, 240(SI)

	// Stage 3: dist 4 - 2 blocks of 8
	MOVQ R8, SI
	MOVQ $2, CX
size16_sse2_128_fwd_stage3_loop:
	MOVQ $4, DX
size16_sse2_128_fwd_stage3_inner:
	MOVUPD (SI), X0
	MOVUPD 64(SI), X1 // x
	MOVQ $4, AX; SUBQ DX, AX; SHLQ $1, AX // k = 0, 2, 4, 6
	SHLQ $4, AX // Offset = k * 16
	MOVUPD (R10)(AX*1), X10 // w
	MOVAPD X1, X2; UNPCKLPD X2, X2; MULPD X10, X2
	MOVAPD X1, X3; UNPCKHPD X3, X3; MOVAPD X10, X4; SHUFPD $1, X4, X4; MULPD X3, X4
	XORPD ·maskNegLoPD(SB), X4; ADDPD X4, X2 // t
	MOVAPD X0, X3; ADDPD X2, X0; SUBPD X2, X3
	MOVUPD X0, (SI)
	MOVUPD X3, 64(SI)
	ADDQ $16, SI
	DECQ DX
	JNZ size16_sse2_128_fwd_stage3_inner
	ADDQ $64, SI
	DECQ CX
	JNZ size16_sse2_128_fwd_stage3_loop

	// Stage 4: dist 8 - 1 block of 16
	MOVQ R8, SI
	MOVQ $8, DX
size16_sse2_128_fwd_stage4_inner:
	MOVUPD (SI), X0
	MOVUPD 128(SI), X1
	MOVQ $8, AX; SUBQ DX, AX // k = 0..7
	SHLQ $4, AX // Offset = k * 16
	MOVUPD (R10)(AX*1), X10
	MOVAPD X1, X2; UNPCKLPD X2, X2; MULPD X10, X2
	MOVAPD X1, X3; UNPCKHPD X3, X3; MOVAPD X10, X4; SHUFPD $1, X4, X4; MULPD X3, X4
	XORPD ·maskNegLoPD(SB), X4; ADDPD X4, X2 // t
	MOVAPD X0, X3; ADDPD X2, X0; SUBPD X2, X3
	MOVUPD X0, (SI)
	MOVUPD X3, 128(SI)
	ADDQ $16, SI
	DECQ DX
	JNZ size16_sse2_128_fwd_stage4_inner

	// Copy to dst
	MOVQ dst+0(FP), R14
	CMPQ R8, R14
	JE   size16_sse2_128_fwd_done
	MOVQ $16, CX; MOVQ R8, SI; MOVQ R14, DI
size16_sse2_128_fwd_copy:
	MOVUPD (SI), X0; MOVUPD X0, (DI); ADDQ $16, SI; ADDQ $16, DI; DECQ CX; JNZ size16_sse2_128_fwd_copy

size16_sse2_128_fwd_done:
	MOVB $1, ret+96(FP)
	RET
size16_sse2_128_fwd_err:
	MOVB $0, ret+96(FP)
	RET

// Inverse transform, size 16, complex128, radix-2
TEXT ·InverseSSE2Size16Radix2Complex128Asm(SB), NOSPLIT, $0-97
	MOVQ dst+0(FP), R8
	MOVQ R8, R14
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ src+32(FP), R13

	CMPQ R13, $16
	JNE  size16_sse2_128_inv_err

	CMPQ R8, R9
	JNE  size16_sse2_128_inv_use_dst
	MOVQ R11, R8

size16_sse2_128_inv_use_dst:
	// Stage 1 & 2 (Combined, fully unrolled with inlined bit-reversal)
	// Bit-reversal pattern: 0,8,4,12,2,10,6,14,1,9,5,13,3,11,7,15
	MOVQ R8, SI
	MOVUPS ·maskNegLoPD(SB), X15 // for i

	// Block 0: src[0,8,4,12]
	MOVUPD 0(R9), X0; MOVUPD 128(R9), X1; MOVUPD 64(R9), X2; MOVUPD 192(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 0(SI); MOVUPD X8, 16(SI); MOVUPD X10, 32(SI); MOVUPD X12, 48(SI)

	// Block 1: src[2,10,6,14]
	MOVUPD 32(R9), X0; MOVUPD 160(R9), X1; MOVUPD 96(R9), X2; MOVUPD 224(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 64(SI); MOVUPD X8, 80(SI); MOVUPD X10, 96(SI); MOVUPD X12, 112(SI)

	// Block 2: src[1,9,5,13]
	MOVUPD 16(R9), X0; MOVUPD 144(R9), X1; MOVUPD 80(R9), X2; MOVUPD 208(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 128(SI); MOVUPD X8, 144(SI); MOVUPD X10, 160(SI); MOVUPD X12, 176(SI)

	// Block 3: src[3,11,7,15]
	MOVUPD 48(R9), X0; MOVUPD 176(R9), X1; MOVUPD 112(R9), X2; MOVUPD 240(R9), X3
	MOVAPD X0, X8; ADDPD X1, X0; SUBPD X1, X8
	MOVAPD X2, X9; ADDPD X3, X2; SUBPD X3, X9
	MOVAPD X0, X10; ADDPD X2, X0; SUBPD X2, X10
	MOVAPD X9, X11; SHUFPD $1, X11, X11; XORPD X15, X11
	MOVAPD X8, X12; ADDPD X11, X8; SUBPD X11, X12
	MOVUPD X0, 192(SI); MOVUPD X8, 208(SI); MOVUPD X10, 224(SI); MOVUPD X12, 240(SI)

	MOVUPS ·maskNegHiPD(SB), X14 // for conj

	// Stage 3: dist 4
	MOVQ R8, SI
	MOVQ $2, CX
size16_sse2_128_inv_stage3_loop:
	MOVQ $4, DX
size16_sse2_128_inv_stage3_inner:
	MOVUPD (SI), X0
	MOVUPD 64(SI), X1
	MOVQ $4, AX; SUBQ DX, AX; SHLQ $1, AX; SHLQ $4, AX; MOVUPD (R10)(AX*1), X10; XORPD X14, X10
	MOVAPD X1, X2; UNPCKLPD X2, X2; MULPD X10, X2
	MOVAPD X1, X3; UNPCKHPD X3, X3; MOVAPD X10, X4; SHUFPD $1, X4, X4; MULPD X3, X4
	XORPD ·maskNegLoPD(SB), X4; ADDPD X4, X2
	MOVAPD X0, X3; ADDPD X2, X0; SUBPD X2, X3
	MOVUPD X0, (SI)
	MOVUPD X3, 64(SI)
	ADDQ $16, SI
	DECQ DX
	JNZ size16_sse2_128_inv_stage3_inner
	ADDQ $64, SI
	DECQ CX
	JNZ size16_sse2_128_inv_stage3_loop

	// Stage 4: dist 8
	MOVQ R8, SI
	MOVQ $8, DX
size16_sse2_128_inv_stage4_inner:
	MOVUPD (SI), X0
	MOVUPD 128(SI), X1
	MOVQ $8, AX; SUBQ DX, AX; SHLQ $4, AX; MOVUPD (R10)(AX*1), X10; XORPD X14, X10
	MOVAPD X1, X2; UNPCKLPD X2, X2; MULPD X10, X2
	MOVAPD X1, X3; UNPCKHPD X3, X3; MOVAPD X10, X4; SHUFPD $1, X4, X4; MULPD X3, X4
	XORPD ·maskNegLoPD(SB), X4; ADDPD X4, X2
	MOVAPD X0, X3; ADDPD X2, X0; SUBPD X2, X3
	MOVUPD X0, (SI)
	MOVUPD X3, 128(SI)
	ADDQ $16, SI
	DECQ DX
	JNZ size16_sse2_128_inv_stage4_inner

	// Scale by 1/16
	MOVSD ·sixteenth64(SB), X15; SHUFPD $0, X15, X15
	MOVQ $16, CX; MOVQ R8, SI
size16_sse2_128_inv_scale:
	MOVUPD (SI), X0; MULPD X15, X0; MOVUPD X0, (SI); ADDQ $16, SI; DECQ CX; JNZ size16_sse2_128_inv_scale

	// Copy to dst
	MOVQ dst+0(FP), R14
	CMPQ R8, R14
	JE   size16_sse2_128_inv_done
	MOVQ $16, CX; MOVQ R8, SI; MOVQ R14, DI
size16_sse2_128_inv_copy:
	MOVUPD (SI), X0; MOVUPD X0, (DI); ADDQ $16, SI; ADDQ $16, DI; DECQ CX; JNZ size16_sse2_128_inv_copy

size16_sse2_128_inv_done:
	MOVB $1, ret+96(FP)
	RET
size16_sse2_128_inv_err:
	MOVB $0, ret+96(FP)
	RET
