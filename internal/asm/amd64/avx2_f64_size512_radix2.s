//go:build amd64 && asm && !purego

// ===========================================================================
// AVX2 Size-512 FFT Kernels for AMD64 (complex128)
// ===========================================================================
//
// Size-specific entrypoints for n==512 that use XMM operations for
// correctness and a fixed-size DIT schedule.
//
// Radix-2: 9 stages (512 = 2^9)
//
// ===========================================================================

#include "textflag.h"

// ===========================================================================
// Forward transform, size 512, complex128, radix-2
// ===========================================================================
TEXT ·ForwardAVX2Size512Radix2Complex128Asm(SB), NOSPLIT, $0-97
	// Load parameters
	MOVQ dst+0(FP), R8       // dst pointer
	MOVQ src+24(FP), R9      // src pointer
	MOVQ twiddle+48(FP), R10 // twiddle pointer
	MOVQ scratch+72(FP), R11 // scratch pointer
	MOVQ src+32(FP), R13     // n (should be 512)

	CMPQ R13, $512
	JNE  size512_128_r2_return_false

	// Validate all slice lengths >= 512
	MOVQ dst+8(FP), AX
	CMPQ AX, $512
	JL   size512_128_r2_return_false

	MOVQ twiddle+56(FP), AX
	CMPQ AX, $512
	JL   size512_128_r2_return_false

	MOVQ scratch+80(FP), AX
	CMPQ AX, $512
	JL   size512_128_r2_return_false

	// Select working buffer
	CMPQ R8, R9
	JNE  size512_128_r2_use_dst
	MOVQ R11, R8

size512_128_r2_use_dst:
	// -----------------------------------------------------------------------
	// FUSED: Bit-reversal permutation + Stage 1 (identity twiddles)
	// Unrolled even half, executed twice (second pass uses +16 src, +4096 dst).
	// -----------------------------------------------------------------------
	MOVQ R8, R14
	MOVQ R9, R15
	XORQ BX, BX

size512_128_r2_use_dst_stage1_pass:
	// -----------------------------------------------------------------------
	// Bit-reversal permutation + Stage 1 (identity twiddles)
	// -----------------------------------------------------------------------
	// Bitrev pattern is the 9-bit reversed index order for n=512.
	// Stage 1 butterflies: a' = a + b, b' = a - b (twiddle[0] = 1).
	//
	// TODO: Check if a smaller loop odd/even is more efficient here.

	// (0,256) -> work[0], work[1]
	MOVUPD 0(R9), X0
	MOVUPD 4096(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 0(R8)
	MOVUPD X3, 16(R8)

	// (128,384) -> work[2], work[3]
	MOVUPD 2048(R9), X0
	MOVUPD 6144(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 32(R8)
	MOVUPD X3, 48(R8)

	// (64,320) -> work[4], work[5]
	MOVUPD 1024(R9), X0
	MOVUPD 5120(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 64(R8)
	MOVUPD X3, 80(R8)

	// (192,448) -> work[6], work[7]
	MOVUPD 3072(R9), X0
	MOVUPD 7168(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 96(R8)
	MOVUPD X3, 112(R8)

	// (32,288) -> work[8], work[9]
	MOVUPD 512(R9), X0
	MOVUPD 4608(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 128(R8)
	MOVUPD X3, 144(R8)

	// (160,416) -> work[10], work[11]
	MOVUPD 2560(R9), X0
	MOVUPD 6656(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 160(R8)
	MOVUPD X3, 176(R8)

	// (96,352) -> work[12], work[13]
	MOVUPD 1536(R9), X0
	MOVUPD 5632(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 192(R8)
	MOVUPD X3, 208(R8)

	// (224,480) -> work[14], work[15]
	MOVUPD 3584(R9), X0
	MOVUPD 7680(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 224(R8)
	MOVUPD X3, 240(R8)

	// (16,272) -> work[16], work[17]
	MOVUPD 256(R9), X0
	MOVUPD 4352(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 256(R8)
	MOVUPD X3, 272(R8)

	// (144,400) -> work[18], work[19]
	MOVUPD 2304(R9), X0
	MOVUPD 6400(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 288(R8)
	MOVUPD X3, 304(R8)

	// (80,336) -> work[20], work[21]
	MOVUPD 1280(R9), X0
	MOVUPD 5376(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 320(R8)
	MOVUPD X3, 336(R8)

	// (208,464) -> work[22], work[23]
	MOVUPD 3328(R9), X0
	MOVUPD 7424(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 352(R8)
	MOVUPD X3, 368(R8)

	// (48,304) -> work[24], work[25]
	MOVUPD 768(R9), X0
	MOVUPD 4864(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 384(R8)
	MOVUPD X3, 400(R8)

	// (176,432) -> work[26], work[27]
	MOVUPD 2816(R9), X0
	MOVUPD 6912(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 416(R8)
	MOVUPD X3, 432(R8)

	// (112,368) -> work[28], work[29]
	MOVUPD 1792(R9), X0
	MOVUPD 5888(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 448(R8)
	MOVUPD X3, 464(R8)

	// (240,496) -> work[30], work[31]
	MOVUPD 3840(R9), X0
	MOVUPD 7936(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 480(R8)
	MOVUPD X3, 496(R8)

	// (8,264) -> work[32], work[33]
	MOVUPD 128(R9), X0
	MOVUPD 4224(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 512(R8)
	MOVUPD X3, 528(R8)

	// (136,392) -> work[34], work[35]
	MOVUPD 2176(R9), X0
	MOVUPD 6272(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 544(R8)
	MOVUPD X3, 560(R8)

	// (72,328) -> work[36], work[37]
	MOVUPD 1152(R9), X0
	MOVUPD 5248(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 576(R8)
	MOVUPD X3, 592(R8)

	// (200,456) -> work[38], work[39]
	MOVUPD 3200(R9), X0
	MOVUPD 7296(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 608(R8)
	MOVUPD X3, 624(R8)

	// (40,296) -> work[40], work[41]
	MOVUPD 640(R9), X0
	MOVUPD 4736(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 640(R8)
	MOVUPD X3, 656(R8)

	// (168,424) -> work[42], work[43]
	MOVUPD 2688(R9), X0
	MOVUPD 6784(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 672(R8)
	MOVUPD X3, 688(R8)

	// (104,360) -> work[44], work[45]
	MOVUPD 1664(R9), X0
	MOVUPD 5760(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 704(R8)
	MOVUPD X3, 720(R8)

	// (232,488) -> work[46], work[47]
	MOVUPD 3712(R9), X0
	MOVUPD 7808(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 736(R8)
	MOVUPD X3, 752(R8)

	// (24,280) -> work[48], work[49]
	MOVUPD 384(R9), X0
	MOVUPD 4480(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 768(R8)
	MOVUPD X3, 784(R8)

	// (152,408) -> work[50], work[51]
	MOVUPD 2432(R9), X0
	MOVUPD 6528(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 800(R8)
	MOVUPD X3, 816(R8)

	// (88,344) -> work[52], work[53]
	MOVUPD 1408(R9), X0
	MOVUPD 5504(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 832(R8)
	MOVUPD X3, 848(R8)

	// (216,472) -> work[54], work[55]
	MOVUPD 3456(R9), X0
	MOVUPD 7552(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 864(R8)
	MOVUPD X3, 880(R8)

	// (56,312) -> work[56], work[57]
	MOVUPD 896(R9), X0
	MOVUPD 4992(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 896(R8)
	MOVUPD X3, 912(R8)

	// (184,440) -> work[58], work[59]
	MOVUPD 2944(R9), X0
	MOVUPD 7040(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 928(R8)
	MOVUPD X3, 944(R8)

	// (120,376) -> work[60], work[61]
	MOVUPD 1920(R9), X0
	MOVUPD 6016(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 960(R8)
	MOVUPD X3, 976(R8)

	// (248,504) -> work[62], work[63]
	MOVUPD 3968(R9), X0
	MOVUPD 8064(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 992(R8)
	MOVUPD X3, 1008(R8)

	// (4,260) -> work[64], work[65]
	MOVUPD 64(R9), X0
	MOVUPD 4160(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1024(R8)
	MOVUPD X3, 1040(R8)

	// (132,388) -> work[66], work[67]
	MOVUPD 2112(R9), X0
	MOVUPD 6208(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1056(R8)
	MOVUPD X3, 1072(R8)

	// (68,324) -> work[68], work[69]
	MOVUPD 1088(R9), X0
	MOVUPD 5184(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1088(R8)
	MOVUPD X3, 1104(R8)

	// (196,452) -> work[70], work[71]
	MOVUPD 3136(R9), X0
	MOVUPD 7232(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1120(R8)
	MOVUPD X3, 1136(R8)

	// (36,292) -> work[72], work[73]
	MOVUPD 576(R9), X0
	MOVUPD 4672(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1152(R8)
	MOVUPD X3, 1168(R8)

	// (164,420) -> work[74], work[75]
	MOVUPD 2624(R9), X0
	MOVUPD 6720(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1184(R8)
	MOVUPD X3, 1200(R8)

	// (100,356) -> work[76], work[77]
	MOVUPD 1600(R9), X0
	MOVUPD 5696(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1216(R8)
	MOVUPD X3, 1232(R8)

	// (228,484) -> work[78], work[79]
	MOVUPD 3648(R9), X0
	MOVUPD 7744(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1248(R8)
	MOVUPD X3, 1264(R8)

	// (20,276) -> work[80], work[81]
	MOVUPD 320(R9), X0
	MOVUPD 4416(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1280(R8)
	MOVUPD X3, 1296(R8)

	// (148,404) -> work[82], work[83]
	MOVUPD 2368(R9), X0
	MOVUPD 6464(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1312(R8)
	MOVUPD X3, 1328(R8)

	// (84,340) -> work[84], work[85]
	MOVUPD 1344(R9), X0
	MOVUPD 5440(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1344(R8)
	MOVUPD X3, 1360(R8)

	// (212,468) -> work[86], work[87]
	MOVUPD 3392(R9), X0
	MOVUPD 7488(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1376(R8)
	MOVUPD X3, 1392(R8)

	// (52,308) -> work[88], work[89]
	MOVUPD 832(R9), X0
	MOVUPD 4928(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1408(R8)
	MOVUPD X3, 1424(R8)

	// (180,436) -> work[90], work[91]
	MOVUPD 2880(R9), X0
	MOVUPD 6976(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1440(R8)
	MOVUPD X3, 1456(R8)

	// (116,372) -> work[92], work[93]
	MOVUPD 1856(R9), X0
	MOVUPD 5952(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1472(R8)
	MOVUPD X3, 1488(R8)

	// (244,500) -> work[94], work[95]
	MOVUPD 3904(R9), X0
	MOVUPD 8000(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1504(R8)
	MOVUPD X3, 1520(R8)

	// (12,268) -> work[96], work[97]
	MOVUPD 192(R9), X0
	MOVUPD 4288(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1536(R8)
	MOVUPD X3, 1552(R8)

	// (140,396) -> work[98], work[99]
	MOVUPD 2240(R9), X0
	MOVUPD 6336(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1568(R8)
	MOVUPD X3, 1584(R8)

	// (76,332) -> work[100], work[101]
	MOVUPD 1216(R9), X0
	MOVUPD 5312(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1600(R8)
	MOVUPD X3, 1616(R8)

	// (204,460) -> work[102], work[103]
	MOVUPD 3264(R9), X0
	MOVUPD 7360(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1632(R8)
	MOVUPD X3, 1648(R8)

	// (44,300) -> work[104], work[105]
	MOVUPD 704(R9), X0
	MOVUPD 4800(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1664(R8)
	MOVUPD X3, 1680(R8)

	// (172,428) -> work[106], work[107]
	MOVUPD 2752(R9), X0
	MOVUPD 6848(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1696(R8)
	MOVUPD X3, 1712(R8)

	// (108,364) -> work[108], work[109]
	MOVUPD 1728(R9), X0
	MOVUPD 5824(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1728(R8)
	MOVUPD X3, 1744(R8)

	// (236,492) -> work[110], work[111]
	MOVUPD 3776(R9), X0
	MOVUPD 7872(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1760(R8)
	MOVUPD X3, 1776(R8)

	// (28,284) -> work[112], work[113]
	MOVUPD 448(R9), X0
	MOVUPD 4544(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1792(R8)
	MOVUPD X3, 1808(R8)

	// (156,412) -> work[114], work[115]
	MOVUPD 2496(R9), X0
	MOVUPD 6592(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1824(R8)
	MOVUPD X3, 1840(R8)

	// (92,348) -> work[116], work[117]
	MOVUPD 1472(R9), X0
	MOVUPD 5568(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1856(R8)
	MOVUPD X3, 1872(R8)

	// (220,476) -> work[118], work[119]
	MOVUPD 3520(R9), X0
	MOVUPD 7616(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1888(R8)
	MOVUPD X3, 1904(R8)

	// (60,316) -> work[120], work[121]
	MOVUPD 960(R9), X0
	MOVUPD 5056(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1920(R8)
	MOVUPD X3, 1936(R8)

	// (188,444) -> work[122], work[123]
	MOVUPD 3008(R9), X0
	MOVUPD 7104(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1952(R8)
	MOVUPD X3, 1968(R8)

	// (124,380) -> work[124], work[125]
	MOVUPD 1984(R9), X0
	MOVUPD 6080(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1984(R8)
	MOVUPD X3, 2000(R8)

	// (252,508) -> work[126], work[127]
	MOVUPD 4032(R9), X0
	MOVUPD 8128(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2016(R8)
	MOVUPD X3, 2032(R8)

	// (2,258) -> work[128], work[129]
	MOVUPD 32(R9), X0
	MOVUPD 4128(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2048(R8)
	MOVUPD X3, 2064(R8)

	// (130,386) -> work[130], work[131]
	MOVUPD 2080(R9), X0
	MOVUPD 6176(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2080(R8)
	MOVUPD X3, 2096(R8)

	// (66,322) -> work[132], work[133]
	MOVUPD 1056(R9), X0
	MOVUPD 5152(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2112(R8)
	MOVUPD X3, 2128(R8)

	// (194,450) -> work[134], work[135]
	MOVUPD 3104(R9), X0
	MOVUPD 7200(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2144(R8)
	MOVUPD X3, 2160(R8)

	// (34,290) -> work[136], work[137]
	MOVUPD 544(R9), X0
	MOVUPD 4640(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2176(R8)
	MOVUPD X3, 2192(R8)

	// (162,418) -> work[138], work[139]
	MOVUPD 2592(R9), X0
	MOVUPD 6688(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2208(R8)
	MOVUPD X3, 2224(R8)

	// (98,354) -> work[140], work[141]
	MOVUPD 1568(R9), X0
	MOVUPD 5664(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2240(R8)
	MOVUPD X3, 2256(R8)

	// (226,482) -> work[142], work[143]
	MOVUPD 3616(R9), X0
	MOVUPD 7712(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2272(R8)
	MOVUPD X3, 2288(R8)

	// (18,274) -> work[144], work[145]
	MOVUPD 288(R9), X0
	MOVUPD 4384(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2304(R8)
	MOVUPD X3, 2320(R8)

	// (146,402) -> work[146], work[147]
	MOVUPD 2336(R9), X0
	MOVUPD 6432(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2336(R8)
	MOVUPD X3, 2352(R8)

	// (82,338) -> work[148], work[149]
	MOVUPD 1312(R9), X0
	MOVUPD 5408(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2368(R8)
	MOVUPD X3, 2384(R8)

	// (210,466) -> work[150], work[151]
	MOVUPD 3360(R9), X0
	MOVUPD 7456(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2400(R8)
	MOVUPD X3, 2416(R8)

	// (50,306) -> work[152], work[153]
	MOVUPD 800(R9), X0
	MOVUPD 4896(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2432(R8)
	MOVUPD X3, 2448(R8)

	// (178,434) -> work[154], work[155]
	MOVUPD 2848(R9), X0
	MOVUPD 6944(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2464(R8)
	MOVUPD X3, 2480(R8)

	// (114,370) -> work[156], work[157]
	MOVUPD 1824(R9), X0
	MOVUPD 5920(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2496(R8)
	MOVUPD X3, 2512(R8)

	// (242,498) -> work[158], work[159]
	MOVUPD 3872(R9), X0
	MOVUPD 7968(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2528(R8)
	MOVUPD X3, 2544(R8)

	// (10,266) -> work[160], work[161]
	MOVUPD 160(R9), X0
	MOVUPD 4256(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2560(R8)
	MOVUPD X3, 2576(R8)

	// (138,394) -> work[162], work[163]
	MOVUPD 2208(R9), X0
	MOVUPD 6304(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2592(R8)
	MOVUPD X3, 2608(R8)

	// (74,330) -> work[164], work[165]
	MOVUPD 1184(R9), X0
	MOVUPD 5280(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2624(R8)
	MOVUPD X3, 2640(R8)

	// (202,458) -> work[166], work[167]
	MOVUPD 3232(R9), X0
	MOVUPD 7328(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2656(R8)
	MOVUPD X3, 2672(R8)

	// (42,298) -> work[168], work[169]
	MOVUPD 672(R9), X0
	MOVUPD 4768(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2688(R8)
	MOVUPD X3, 2704(R8)

	// (170,426) -> work[170], work[171]
	MOVUPD 2720(R9), X0
	MOVUPD 6816(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2720(R8)
	MOVUPD X3, 2736(R8)

	// (106,362) -> work[172], work[173]
	MOVUPD 1696(R9), X0
	MOVUPD 5792(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2752(R8)
	MOVUPD X3, 2768(R8)

	// (234,490) -> work[174], work[175]
	MOVUPD 3744(R9), X0
	MOVUPD 7840(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2784(R8)
	MOVUPD X3, 2800(R8)

	// (26,282) -> work[176], work[177]
	MOVUPD 416(R9), X0
	MOVUPD 4512(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2816(R8)
	MOVUPD X3, 2832(R8)

	// (154,410) -> work[178], work[179]
	MOVUPD 2464(R9), X0
	MOVUPD 6560(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2848(R8)
	MOVUPD X3, 2864(R8)

	// (90,346) -> work[180], work[181]
	MOVUPD 1440(R9), X0
	MOVUPD 5536(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2880(R8)
	MOVUPD X3, 2896(R8)

	// (218,474) -> work[182], work[183]
	MOVUPD 3488(R9), X0
	MOVUPD 7584(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2912(R8)
	MOVUPD X3, 2928(R8)

	// (58,314) -> work[184], work[185]
	MOVUPD 928(R9), X0
	MOVUPD 5024(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2944(R8)
	MOVUPD X3, 2960(R8)

	// (186,442) -> work[186], work[187]
	MOVUPD 2976(R9), X0
	MOVUPD 7072(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2976(R8)
	MOVUPD X3, 2992(R8)

	// (122,378) -> work[188], work[189]
	MOVUPD 1952(R9), X0
	MOVUPD 6048(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3008(R8)
	MOVUPD X3, 3024(R8)

	// (250,506) -> work[190], work[191]
	MOVUPD 4000(R9), X0
	MOVUPD 8096(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3040(R8)
	MOVUPD X3, 3056(R8)

	// (6,262) -> work[192], work[193]
	MOVUPD 96(R9), X0
	MOVUPD 4192(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3072(R8)
	MOVUPD X3, 3088(R8)

	// (134,390) -> work[194], work[195]
	MOVUPD 2144(R9), X0
	MOVUPD 6240(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3104(R8)
	MOVUPD X3, 3120(R8)

	// (70,326) -> work[196], work[197]
	MOVUPD 1120(R9), X0
	MOVUPD 5216(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3136(R8)
	MOVUPD X3, 3152(R8)

	// (198,454) -> work[198], work[199]
	MOVUPD 3168(R9), X0
	MOVUPD 7264(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3168(R8)
	MOVUPD X3, 3184(R8)

	// (38,294) -> work[200], work[201]
	MOVUPD 608(R9), X0
	MOVUPD 4704(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3200(R8)
	MOVUPD X3, 3216(R8)

	// (166,422) -> work[202], work[203]
	MOVUPD 2656(R9), X0
	MOVUPD 6752(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3232(R8)
	MOVUPD X3, 3248(R8)

	// (102,358) -> work[204], work[205]
	MOVUPD 1632(R9), X0
	MOVUPD 5728(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3264(R8)
	MOVUPD X3, 3280(R8)

	// (230,486) -> work[206], work[207]
	MOVUPD 3680(R9), X0
	MOVUPD 7776(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3296(R8)
	MOVUPD X3, 3312(R8)

	// (22,278) -> work[208], work[209]
	MOVUPD 352(R9), X0
	MOVUPD 4448(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3328(R8)
	MOVUPD X3, 3344(R8)

	// (150,406) -> work[210], work[211]
	MOVUPD 2400(R9), X0
	MOVUPD 6496(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3360(R8)
	MOVUPD X3, 3376(R8)

	// (86,342) -> work[212], work[213]
	MOVUPD 1376(R9), X0
	MOVUPD 5472(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3392(R8)
	MOVUPD X3, 3408(R8)

	// (214,470) -> work[214], work[215]
	MOVUPD 3424(R9), X0
	MOVUPD 7520(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3424(R8)
	MOVUPD X3, 3440(R8)

	// (54,310) -> work[216], work[217]
	MOVUPD 864(R9), X0
	MOVUPD 4960(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3456(R8)
	MOVUPD X3, 3472(R8)

	// (182,438) -> work[218], work[219]
	MOVUPD 2912(R9), X0
	MOVUPD 7008(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3488(R8)
	MOVUPD X3, 3504(R8)

	// (118,374) -> work[220], work[221]
	MOVUPD 1888(R9), X0
	MOVUPD 5984(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3520(R8)
	MOVUPD X3, 3536(R8)

	// (246,502) -> work[222], work[223]
	MOVUPD 3936(R9), X0
	MOVUPD 8032(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3552(R8)
	MOVUPD X3, 3568(R8)

	// (14,270) -> work[224], work[225]
	MOVUPD 224(R9), X0
	MOVUPD 4320(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3584(R8)
	MOVUPD X3, 3600(R8)

	// (142,398) -> work[226], work[227]
	MOVUPD 2272(R9), X0
	MOVUPD 6368(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3616(R8)
	MOVUPD X3, 3632(R8)

	// (78,334) -> work[228], work[229]
	MOVUPD 1248(R9), X0
	MOVUPD 5344(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3648(R8)
	MOVUPD X3, 3664(R8)

	// (206,462) -> work[230], work[231]
	MOVUPD 3296(R9), X0
	MOVUPD 7392(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3680(R8)
	MOVUPD X3, 3696(R8)

	// (46,302) -> work[232], work[233]
	MOVUPD 736(R9), X0
	MOVUPD 4832(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3712(R8)
	MOVUPD X3, 3728(R8)

	// (174,430) -> work[234], work[235]
	MOVUPD 2784(R9), X0
	MOVUPD 6880(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3744(R8)
	MOVUPD X3, 3760(R8)

	// (110,366) -> work[236], work[237]
	MOVUPD 1760(R9), X0
	MOVUPD 5856(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3776(R8)
	MOVUPD X3, 3792(R8)

	// (238,494) -> work[238], work[239]
	MOVUPD 3808(R9), X0
	MOVUPD 7904(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3808(R8)
	MOVUPD X3, 3824(R8)

	// (30,286) -> work[240], work[241]
	MOVUPD 480(R9), X0
	MOVUPD 4576(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3840(R8)
	MOVUPD X3, 3856(R8)

	// (158,414) -> work[242], work[243]
	MOVUPD 2528(R9), X0
	MOVUPD 6624(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3872(R8)
	MOVUPD X3, 3888(R8)

	// (94,350) -> work[244], work[245]
	MOVUPD 1504(R9), X0
	MOVUPD 5600(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3904(R8)
	MOVUPD X3, 3920(R8)

	// (222,478) -> work[246], work[247]
	MOVUPD 3552(R9), X0
	MOVUPD 7648(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3936(R8)
	MOVUPD X3, 3952(R8)

	// (62,318) -> work[248], work[249]
	MOVUPD 992(R9), X0
	MOVUPD 5088(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3968(R8)
	MOVUPD X3, 3984(R8)

	// (190,446) -> work[250], work[251]
	MOVUPD 3040(R9), X0
	MOVUPD 7136(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4000(R8)
	MOVUPD X3, 4016(R8)

	// (126,382) -> work[252], work[253]
	MOVUPD 2016(R9), X0
	MOVUPD 6112(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4032(R8)
	MOVUPD X3, 4048(R8)

	// (254,510) -> work[254], work[255]
	MOVUPD 4064(R9), X0
	MOVUPD 8160(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4064(R8)
	MOVUPD X3, 4080(R8)
	INCQ BX
	CMPQ BX, $2
	JGE  size512_128_r2_use_dst_stage1_done
	LEAQ 4096(R14), R8
	LEAQ 16(R15), R9
	JMP  size512_128_r2_use_dst_stage1_pass

size512_128_r2_use_dst_stage1_done:
	MOVQ R14, R8

size512_128_r2_stage2:
	// Stage 2: size=4, half=2, step=128
	MOVQ $128, BX
	XORQ CX, CX

size512_128_r2_stage2_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage3

	XORQ DX, DX

size512_128_r2_stage2_j:
	CMPQ DX, $2
	JGE  size512_128_r2_stage2_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $2, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage2_j

size512_128_r2_stage2_next:
	ADDQ $4, CX
	JMP  size512_128_r2_stage2_base

size512_128_r2_stage3:
	// Stage 3: size=8, half=4, step=64
	MOVQ $64, BX
	XORQ CX, CX

size512_128_r2_stage3_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage4

	XORQ DX, DX

size512_128_r2_stage3_j:
	CMPQ DX, $4
	JGE  size512_128_r2_stage3_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $4, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage3_j

size512_128_r2_stage3_next:
	ADDQ $8, CX
	JMP  size512_128_r2_stage3_base

size512_128_r2_stage4:
	// Stage 4: size=16, half=8, step=32
	MOVQ $32, BX
	XORQ CX, CX

size512_128_r2_stage4_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage5

	XORQ DX, DX

size512_128_r2_stage4_j:
	CMPQ DX, $8
	JGE  size512_128_r2_stage4_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $8, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage4_j

size512_128_r2_stage4_next:
	ADDQ $16, CX
	JMP  size512_128_r2_stage4_base

size512_128_r2_stage5:
	// Stage 5: size=32, half=16, step=16
	MOVQ $16, BX
	XORQ CX, CX

size512_128_r2_stage5_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage6

	XORQ DX, DX

size512_128_r2_stage5_j:
	CMPQ DX, $16
	JGE  size512_128_r2_stage5_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $16, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage5_j

size512_128_r2_stage5_next:
	ADDQ $32, CX
	JMP  size512_128_r2_stage5_base

size512_128_r2_stage6:
	// Stage 6: size=64, half=32, step=8
	MOVQ $8, BX
	XORQ CX, CX

size512_128_r2_stage6_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage7

	XORQ DX, DX

size512_128_r2_stage6_j:
	CMPQ DX, $32
	JGE  size512_128_r2_stage6_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $32, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage6_j

size512_128_r2_stage6_next:
	ADDQ $64, CX
	JMP  size512_128_r2_stage6_base

size512_128_r2_stage7:
	// Stage 7: size=128, half=64, step=4
	MOVQ $4, BX
	XORQ CX, CX

size512_128_r2_stage7_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage8

	XORQ DX, DX

size512_128_r2_stage7_j:
	CMPQ DX, $64
	JGE  size512_128_r2_stage7_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $64, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage7_j

size512_128_r2_stage7_next:
	ADDQ $128, CX
	JMP  size512_128_r2_stage7_base

size512_128_r2_stage8:
	// Stage 8: size=256, half=128, step=2
	MOVQ $2, BX
	XORQ CX, CX

size512_128_r2_stage8_base:
	CMPQ CX, $512
	JGE  size512_128_r2_stage9

	XORQ DX, DX

size512_128_r2_stage8_j:
	CMPQ DX, $128
	JGE  size512_128_r2_stage8_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $128, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage8_j

size512_128_r2_stage8_next:
	ADDQ $256, CX
	JMP  size512_128_r2_stage8_base

size512_128_r2_stage9:
	// Stage 9: size=512, half=256, step=1
	MOVQ $1, BX
	XORQ CX, CX
	XORQ DX, DX

size512_128_r2_stage9_j:
	CMPQ DX, $256
	JGE  size512_128_r2_finalize

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $256, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMADDSUB231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_128_r2_stage9_j

size512_128_r2_finalize:
	// Copy results to dst if needed
	MOVQ dst+0(FP), R9
	CMPQ R8, R9
	JE   size512_128_r2_done

	XORQ CX, CX
size512_128_r2_copy_loop:
	VMOVUPS (R8)(CX*1), Y0
	VMOVUPS Y0, (R9)(CX*1)
	ADDQ $32, CX
	CMPQ CX, $8192           // 512 * 16 bytes
	JL   size512_128_r2_copy_loop

size512_128_r2_done:
	VZEROUPPER
	MOVB $1, ret+96(FP)
	RET

size512_128_r2_return_false:
	VZEROUPPER
	MOVB $0, ret+96(FP)
	RET

// ===========================================================================
// Inverse transform, size 512, complex128, radix-2
// ===========================================================================
TEXT ·InverseAVX2Size512Radix2Complex128Asm(SB), NOSPLIT, $0-97
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ src+32(FP), R13

	CMPQ R13, $512
	JNE  size512_inv_128_r2_return_false

	MOVQ dst+8(FP), AX
	CMPQ AX, $512
	JL   size512_inv_128_r2_return_false

	MOVQ twiddle+56(FP), AX
	CMPQ AX, $512
	JL   size512_inv_128_r2_return_false

	MOVQ scratch+80(FP), AX
	CMPQ AX, $512
	JL   size512_inv_128_r2_return_false

	CMPQ R8, R9
	JNE  size512_inv_128_r2_use_dst
	MOVQ R11, R8

size512_inv_128_r2_use_dst:
	// -----------------------------------------------------------------------
	// FUSED: Bit-reversal permutation + Stage 1 (identity twiddles)
	// Unrolled even half, executed twice (second pass uses +16 src, +4096 dst).
	// -----------------------------------------------------------------------
	MOVQ R8, R14
	MOVQ R9, R15
	XORQ BX, BX

size512_inv_128_r2_use_dst_stage1_pass:
	// -----------------------------------------------------------------------
	// Bit-reversal permutation + Stage 1 (identity twiddles)
	// -----------------------------------------------------------------------
	// Bitrev pattern is the 9-bit reversed index order for n=512.
	// Stage 1 butterflies: a' = a + b, b' = a - b (twiddle[0] = 1).
	//
	// TODO: Check if a smaller loop odd/even is more efficient here.

	// (0,256) -> work[0], work[1]
	MOVUPD 0(R9), X0
	MOVUPD 4096(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 0(R8)
	MOVUPD X3, 16(R8)

	// (128,384) -> work[2], work[3]
	MOVUPD 2048(R9), X0
	MOVUPD 6144(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 32(R8)
	MOVUPD X3, 48(R8)

	// (64,320) -> work[4], work[5]
	MOVUPD 1024(R9), X0
	MOVUPD 5120(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 64(R8)
	MOVUPD X3, 80(R8)

	// (192,448) -> work[6], work[7]
	MOVUPD 3072(R9), X0
	MOVUPD 7168(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 96(R8)
	MOVUPD X3, 112(R8)

	// (32,288) -> work[8], work[9]
	MOVUPD 512(R9), X0
	MOVUPD 4608(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 128(R8)
	MOVUPD X3, 144(R8)

	// (160,416) -> work[10], work[11]
	MOVUPD 2560(R9), X0
	MOVUPD 6656(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 160(R8)
	MOVUPD X3, 176(R8)

	// (96,352) -> work[12], work[13]
	MOVUPD 1536(R9), X0
	MOVUPD 5632(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 192(R8)
	MOVUPD X3, 208(R8)

	// (224,480) -> work[14], work[15]
	MOVUPD 3584(R9), X0
	MOVUPD 7680(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 224(R8)
	MOVUPD X3, 240(R8)

	// (16,272) -> work[16], work[17]
	MOVUPD 256(R9), X0
	MOVUPD 4352(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 256(R8)
	MOVUPD X3, 272(R8)

	// (144,400) -> work[18], work[19]
	MOVUPD 2304(R9), X0
	MOVUPD 6400(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 288(R8)
	MOVUPD X3, 304(R8)

	// (80,336) -> work[20], work[21]
	MOVUPD 1280(R9), X0
	MOVUPD 5376(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 320(R8)
	MOVUPD X3, 336(R8)

	// (208,464) -> work[22], work[23]
	MOVUPD 3328(R9), X0
	MOVUPD 7424(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 352(R8)
	MOVUPD X3, 368(R8)

	// (48,304) -> work[24], work[25]
	MOVUPD 768(R9), X0
	MOVUPD 4864(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 384(R8)
	MOVUPD X3, 400(R8)

	// (176,432) -> work[26], work[27]
	MOVUPD 2816(R9), X0
	MOVUPD 6912(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 416(R8)
	MOVUPD X3, 432(R8)

	// (112,368) -> work[28], work[29]
	MOVUPD 1792(R9), X0
	MOVUPD 5888(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 448(R8)
	MOVUPD X3, 464(R8)

	// (240,496) -> work[30], work[31]
	MOVUPD 3840(R9), X0
	MOVUPD 7936(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 480(R8)
	MOVUPD X3, 496(R8)

	// (8,264) -> work[32], work[33]
	MOVUPD 128(R9), X0
	MOVUPD 4224(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 512(R8)
	MOVUPD X3, 528(R8)

	// (136,392) -> work[34], work[35]
	MOVUPD 2176(R9), X0
	MOVUPD 6272(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 544(R8)
	MOVUPD X3, 560(R8)

	// (72,328) -> work[36], work[37]
	MOVUPD 1152(R9), X0
	MOVUPD 5248(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 576(R8)
	MOVUPD X3, 592(R8)

	// (200,456) -> work[38], work[39]
	MOVUPD 3200(R9), X0
	MOVUPD 7296(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 608(R8)
	MOVUPD X3, 624(R8)

	// (40,296) -> work[40], work[41]
	MOVUPD 640(R9), X0
	MOVUPD 4736(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 640(R8)
	MOVUPD X3, 656(R8)

	// (168,424) -> work[42], work[43]
	MOVUPD 2688(R9), X0
	MOVUPD 6784(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 672(R8)
	MOVUPD X3, 688(R8)

	// (104,360) -> work[44], work[45]
	MOVUPD 1664(R9), X0
	MOVUPD 5760(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 704(R8)
	MOVUPD X3, 720(R8)

	// (232,488) -> work[46], work[47]
	MOVUPD 3712(R9), X0
	MOVUPD 7808(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 736(R8)
	MOVUPD X3, 752(R8)

	// (24,280) -> work[48], work[49]
	MOVUPD 384(R9), X0
	MOVUPD 4480(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 768(R8)
	MOVUPD X3, 784(R8)

	// (152,408) -> work[50], work[51]
	MOVUPD 2432(R9), X0
	MOVUPD 6528(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 800(R8)
	MOVUPD X3, 816(R8)

	// (88,344) -> work[52], work[53]
	MOVUPD 1408(R9), X0
	MOVUPD 5504(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 832(R8)
	MOVUPD X3, 848(R8)

	// (216,472) -> work[54], work[55]
	MOVUPD 3456(R9), X0
	MOVUPD 7552(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 864(R8)
	MOVUPD X3, 880(R8)

	// (56,312) -> work[56], work[57]
	MOVUPD 896(R9), X0
	MOVUPD 4992(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 896(R8)
	MOVUPD X3, 912(R8)

	// (184,440) -> work[58], work[59]
	MOVUPD 2944(R9), X0
	MOVUPD 7040(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 928(R8)
	MOVUPD X3, 944(R8)

	// (120,376) -> work[60], work[61]
	MOVUPD 1920(R9), X0
	MOVUPD 6016(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 960(R8)
	MOVUPD X3, 976(R8)

	// (248,504) -> work[62], work[63]
	MOVUPD 3968(R9), X0
	MOVUPD 8064(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 992(R8)
	MOVUPD X3, 1008(R8)

	// (4,260) -> work[64], work[65]
	MOVUPD 64(R9), X0
	MOVUPD 4160(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1024(R8)
	MOVUPD X3, 1040(R8)

	// (132,388) -> work[66], work[67]
	MOVUPD 2112(R9), X0
	MOVUPD 6208(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1056(R8)
	MOVUPD X3, 1072(R8)

	// (68,324) -> work[68], work[69]
	MOVUPD 1088(R9), X0
	MOVUPD 5184(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1088(R8)
	MOVUPD X3, 1104(R8)

	// (196,452) -> work[70], work[71]
	MOVUPD 3136(R9), X0
	MOVUPD 7232(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1120(R8)
	MOVUPD X3, 1136(R8)

	// (36,292) -> work[72], work[73]
	MOVUPD 576(R9), X0
	MOVUPD 4672(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1152(R8)
	MOVUPD X3, 1168(R8)

	// (164,420) -> work[74], work[75]
	MOVUPD 2624(R9), X0
	MOVUPD 6720(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1184(R8)
	MOVUPD X3, 1200(R8)

	// (100,356) -> work[76], work[77]
	MOVUPD 1600(R9), X0
	MOVUPD 5696(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1216(R8)
	MOVUPD X3, 1232(R8)

	// (228,484) -> work[78], work[79]
	MOVUPD 3648(R9), X0
	MOVUPD 7744(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1248(R8)
	MOVUPD X3, 1264(R8)

	// (20,276) -> work[80], work[81]
	MOVUPD 320(R9), X0
	MOVUPD 4416(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1280(R8)
	MOVUPD X3, 1296(R8)

	// (148,404) -> work[82], work[83]
	MOVUPD 2368(R9), X0
	MOVUPD 6464(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1312(R8)
	MOVUPD X3, 1328(R8)

	// (84,340) -> work[84], work[85]
	MOVUPD 1344(R9), X0
	MOVUPD 5440(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1344(R8)
	MOVUPD X3, 1360(R8)

	// (212,468) -> work[86], work[87]
	MOVUPD 3392(R9), X0
	MOVUPD 7488(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1376(R8)
	MOVUPD X3, 1392(R8)

	// (52,308) -> work[88], work[89]
	MOVUPD 832(R9), X0
	MOVUPD 4928(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1408(R8)
	MOVUPD X3, 1424(R8)

	// (180,436) -> work[90], work[91]
	MOVUPD 2880(R9), X0
	MOVUPD 6976(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1440(R8)
	MOVUPD X3, 1456(R8)

	// (116,372) -> work[92], work[93]
	MOVUPD 1856(R9), X0
	MOVUPD 5952(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1472(R8)
	MOVUPD X3, 1488(R8)

	// (244,500) -> work[94], work[95]
	MOVUPD 3904(R9), X0
	MOVUPD 8000(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1504(R8)
	MOVUPD X3, 1520(R8)

	// (12,268) -> work[96], work[97]
	MOVUPD 192(R9), X0
	MOVUPD 4288(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1536(R8)
	MOVUPD X3, 1552(R8)

	// (140,396) -> work[98], work[99]
	MOVUPD 2240(R9), X0
	MOVUPD 6336(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1568(R8)
	MOVUPD X3, 1584(R8)

	// (76,332) -> work[100], work[101]
	MOVUPD 1216(R9), X0
	MOVUPD 5312(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1600(R8)
	MOVUPD X3, 1616(R8)

	// (204,460) -> work[102], work[103]
	MOVUPD 3264(R9), X0
	MOVUPD 7360(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1632(R8)
	MOVUPD X3, 1648(R8)

	// (44,300) -> work[104], work[105]
	MOVUPD 704(R9), X0
	MOVUPD 4800(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1664(R8)
	MOVUPD X3, 1680(R8)

	// (172,428) -> work[106], work[107]
	MOVUPD 2752(R9), X0
	MOVUPD 6848(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1696(R8)
	MOVUPD X3, 1712(R8)

	// (108,364) -> work[108], work[109]
	MOVUPD 1728(R9), X0
	MOVUPD 5824(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1728(R8)
	MOVUPD X3, 1744(R8)

	// (236,492) -> work[110], work[111]
	MOVUPD 3776(R9), X0
	MOVUPD 7872(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1760(R8)
	MOVUPD X3, 1776(R8)

	// (28,284) -> work[112], work[113]
	MOVUPD 448(R9), X0
	MOVUPD 4544(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1792(R8)
	MOVUPD X3, 1808(R8)

	// (156,412) -> work[114], work[115]
	MOVUPD 2496(R9), X0
	MOVUPD 6592(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1824(R8)
	MOVUPD X3, 1840(R8)

	// (92,348) -> work[116], work[117]
	MOVUPD 1472(R9), X0
	MOVUPD 5568(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1856(R8)
	MOVUPD X3, 1872(R8)

	// (220,476) -> work[118], work[119]
	MOVUPD 3520(R9), X0
	MOVUPD 7616(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1888(R8)
	MOVUPD X3, 1904(R8)

	// (60,316) -> work[120], work[121]
	MOVUPD 960(R9), X0
	MOVUPD 5056(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1920(R8)
	MOVUPD X3, 1936(R8)

	// (188,444) -> work[122], work[123]
	MOVUPD 3008(R9), X0
	MOVUPD 7104(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1952(R8)
	MOVUPD X3, 1968(R8)

	// (124,380) -> work[124], work[125]
	MOVUPD 1984(R9), X0
	MOVUPD 6080(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 1984(R8)
	MOVUPD X3, 2000(R8)

	// (252,508) -> work[126], work[127]
	MOVUPD 4032(R9), X0
	MOVUPD 8128(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2016(R8)
	MOVUPD X3, 2032(R8)

	// (2,258) -> work[128], work[129]
	MOVUPD 32(R9), X0
	MOVUPD 4128(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2048(R8)
	MOVUPD X3, 2064(R8)

	// (130,386) -> work[130], work[131]
	MOVUPD 2080(R9), X0
	MOVUPD 6176(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2080(R8)
	MOVUPD X3, 2096(R8)

	// (66,322) -> work[132], work[133]
	MOVUPD 1056(R9), X0
	MOVUPD 5152(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2112(R8)
	MOVUPD X3, 2128(R8)

	// (194,450) -> work[134], work[135]
	MOVUPD 3104(R9), X0
	MOVUPD 7200(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2144(R8)
	MOVUPD X3, 2160(R8)

	// (34,290) -> work[136], work[137]
	MOVUPD 544(R9), X0
	MOVUPD 4640(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2176(R8)
	MOVUPD X3, 2192(R8)

	// (162,418) -> work[138], work[139]
	MOVUPD 2592(R9), X0
	MOVUPD 6688(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2208(R8)
	MOVUPD X3, 2224(R8)

	// (98,354) -> work[140], work[141]
	MOVUPD 1568(R9), X0
	MOVUPD 5664(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2240(R8)
	MOVUPD X3, 2256(R8)

	// (226,482) -> work[142], work[143]
	MOVUPD 3616(R9), X0
	MOVUPD 7712(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2272(R8)
	MOVUPD X3, 2288(R8)

	// (18,274) -> work[144], work[145]
	MOVUPD 288(R9), X0
	MOVUPD 4384(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2304(R8)
	MOVUPD X3, 2320(R8)

	// (146,402) -> work[146], work[147]
	MOVUPD 2336(R9), X0
	MOVUPD 6432(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2336(R8)
	MOVUPD X3, 2352(R8)

	// (82,338) -> work[148], work[149]
	MOVUPD 1312(R9), X0
	MOVUPD 5408(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2368(R8)
	MOVUPD X3, 2384(R8)

	// (210,466) -> work[150], work[151]
	MOVUPD 3360(R9), X0
	MOVUPD 7456(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2400(R8)
	MOVUPD X3, 2416(R8)

	// (50,306) -> work[152], work[153]
	MOVUPD 800(R9), X0
	MOVUPD 4896(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2432(R8)
	MOVUPD X3, 2448(R8)

	// (178,434) -> work[154], work[155]
	MOVUPD 2848(R9), X0
	MOVUPD 6944(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2464(R8)
	MOVUPD X3, 2480(R8)

	// (114,370) -> work[156], work[157]
	MOVUPD 1824(R9), X0
	MOVUPD 5920(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2496(R8)
	MOVUPD X3, 2512(R8)

	// (242,498) -> work[158], work[159]
	MOVUPD 3872(R9), X0
	MOVUPD 7968(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2528(R8)
	MOVUPD X3, 2544(R8)

	// (10,266) -> work[160], work[161]
	MOVUPD 160(R9), X0
	MOVUPD 4256(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2560(R8)
	MOVUPD X3, 2576(R8)

	// (138,394) -> work[162], work[163]
	MOVUPD 2208(R9), X0
	MOVUPD 6304(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2592(R8)
	MOVUPD X3, 2608(R8)

	// (74,330) -> work[164], work[165]
	MOVUPD 1184(R9), X0
	MOVUPD 5280(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2624(R8)
	MOVUPD X3, 2640(R8)

	// (202,458) -> work[166], work[167]
	MOVUPD 3232(R9), X0
	MOVUPD 7328(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2656(R8)
	MOVUPD X3, 2672(R8)

	// (42,298) -> work[168], work[169]
	MOVUPD 672(R9), X0
	MOVUPD 4768(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2688(R8)
	MOVUPD X3, 2704(R8)

	// (170,426) -> work[170], work[171]
	MOVUPD 2720(R9), X0
	MOVUPD 6816(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2720(R8)
	MOVUPD X3, 2736(R8)

	// (106,362) -> work[172], work[173]
	MOVUPD 1696(R9), X0
	MOVUPD 5792(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2752(R8)
	MOVUPD X3, 2768(R8)

	// (234,490) -> work[174], work[175]
	MOVUPD 3744(R9), X0
	MOVUPD 7840(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2784(R8)
	MOVUPD X3, 2800(R8)

	// (26,282) -> work[176], work[177]
	MOVUPD 416(R9), X0
	MOVUPD 4512(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2816(R8)
	MOVUPD X3, 2832(R8)

	// (154,410) -> work[178], work[179]
	MOVUPD 2464(R9), X0
	MOVUPD 6560(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2848(R8)
	MOVUPD X3, 2864(R8)

	// (90,346) -> work[180], work[181]
	MOVUPD 1440(R9), X0
	MOVUPD 5536(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2880(R8)
	MOVUPD X3, 2896(R8)

	// (218,474) -> work[182], work[183]
	MOVUPD 3488(R9), X0
	MOVUPD 7584(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2912(R8)
	MOVUPD X3, 2928(R8)

	// (58,314) -> work[184], work[185]
	MOVUPD 928(R9), X0
	MOVUPD 5024(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2944(R8)
	MOVUPD X3, 2960(R8)

	// (186,442) -> work[186], work[187]
	MOVUPD 2976(R9), X0
	MOVUPD 7072(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 2976(R8)
	MOVUPD X3, 2992(R8)

	// (122,378) -> work[188], work[189]
	MOVUPD 1952(R9), X0
	MOVUPD 6048(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3008(R8)
	MOVUPD X3, 3024(R8)

	// (250,506) -> work[190], work[191]
	MOVUPD 4000(R9), X0
	MOVUPD 8096(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3040(R8)
	MOVUPD X3, 3056(R8)

	// (6,262) -> work[192], work[193]
	MOVUPD 96(R9), X0
	MOVUPD 4192(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3072(R8)
	MOVUPD X3, 3088(R8)

	// (134,390) -> work[194], work[195]
	MOVUPD 2144(R9), X0
	MOVUPD 6240(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3104(R8)
	MOVUPD X3, 3120(R8)

	// (70,326) -> work[196], work[197]
	MOVUPD 1120(R9), X0
	MOVUPD 5216(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3136(R8)
	MOVUPD X3, 3152(R8)

	// (198,454) -> work[198], work[199]
	MOVUPD 3168(R9), X0
	MOVUPD 7264(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3168(R8)
	MOVUPD X3, 3184(R8)

	// (38,294) -> work[200], work[201]
	MOVUPD 608(R9), X0
	MOVUPD 4704(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3200(R8)
	MOVUPD X3, 3216(R8)

	// (166,422) -> work[202], work[203]
	MOVUPD 2656(R9), X0
	MOVUPD 6752(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3232(R8)
	MOVUPD X3, 3248(R8)

	// (102,358) -> work[204], work[205]
	MOVUPD 1632(R9), X0
	MOVUPD 5728(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3264(R8)
	MOVUPD X3, 3280(R8)

	// (230,486) -> work[206], work[207]
	MOVUPD 3680(R9), X0
	MOVUPD 7776(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3296(R8)
	MOVUPD X3, 3312(R8)

	// (22,278) -> work[208], work[209]
	MOVUPD 352(R9), X0
	MOVUPD 4448(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3328(R8)
	MOVUPD X3, 3344(R8)

	// (150,406) -> work[210], work[211]
	MOVUPD 2400(R9), X0
	MOVUPD 6496(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3360(R8)
	MOVUPD X3, 3376(R8)

	// (86,342) -> work[212], work[213]
	MOVUPD 1376(R9), X0
	MOVUPD 5472(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3392(R8)
	MOVUPD X3, 3408(R8)

	// (214,470) -> work[214], work[215]
	MOVUPD 3424(R9), X0
	MOVUPD 7520(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3424(R8)
	MOVUPD X3, 3440(R8)

	// (54,310) -> work[216], work[217]
	MOVUPD 864(R9), X0
	MOVUPD 4960(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3456(R8)
	MOVUPD X3, 3472(R8)

	// (182,438) -> work[218], work[219]
	MOVUPD 2912(R9), X0
	MOVUPD 7008(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3488(R8)
	MOVUPD X3, 3504(R8)

	// (118,374) -> work[220], work[221]
	MOVUPD 1888(R9), X0
	MOVUPD 5984(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3520(R8)
	MOVUPD X3, 3536(R8)

	// (246,502) -> work[222], work[223]
	MOVUPD 3936(R9), X0
	MOVUPD 8032(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3552(R8)
	MOVUPD X3, 3568(R8)

	// (14,270) -> work[224], work[225]
	MOVUPD 224(R9), X0
	MOVUPD 4320(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3584(R8)
	MOVUPD X3, 3600(R8)

	// (142,398) -> work[226], work[227]
	MOVUPD 2272(R9), X0
	MOVUPD 6368(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3616(R8)
	MOVUPD X3, 3632(R8)

	// (78,334) -> work[228], work[229]
	MOVUPD 1248(R9), X0
	MOVUPD 5344(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3648(R8)
	MOVUPD X3, 3664(R8)

	// (206,462) -> work[230], work[231]
	MOVUPD 3296(R9), X0
	MOVUPD 7392(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3680(R8)
	MOVUPD X3, 3696(R8)

	// (46,302) -> work[232], work[233]
	MOVUPD 736(R9), X0
	MOVUPD 4832(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3712(R8)
	MOVUPD X3, 3728(R8)

	// (174,430) -> work[234], work[235]
	MOVUPD 2784(R9), X0
	MOVUPD 6880(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3744(R8)
	MOVUPD X3, 3760(R8)

	// (110,366) -> work[236], work[237]
	MOVUPD 1760(R9), X0
	MOVUPD 5856(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3776(R8)
	MOVUPD X3, 3792(R8)

	// (238,494) -> work[238], work[239]
	MOVUPD 3808(R9), X0
	MOVUPD 7904(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3808(R8)
	MOVUPD X3, 3824(R8)

	// (30,286) -> work[240], work[241]
	MOVUPD 480(R9), X0
	MOVUPD 4576(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3840(R8)
	MOVUPD X3, 3856(R8)

	// (158,414) -> work[242], work[243]
	MOVUPD 2528(R9), X0
	MOVUPD 6624(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3872(R8)
	MOVUPD X3, 3888(R8)

	// (94,350) -> work[244], work[245]
	MOVUPD 1504(R9), X0
	MOVUPD 5600(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3904(R8)
	MOVUPD X3, 3920(R8)

	// (222,478) -> work[246], work[247]
	MOVUPD 3552(R9), X0
	MOVUPD 7648(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3936(R8)
	MOVUPD X3, 3952(R8)

	// (62,318) -> work[248], work[249]
	MOVUPD 992(R9), X0
	MOVUPD 5088(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 3968(R8)
	MOVUPD X3, 3984(R8)

	// (190,446) -> work[250], work[251]
	MOVUPD 3040(R9), X0
	MOVUPD 7136(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4000(R8)
	MOVUPD X3, 4016(R8)

	// (126,382) -> work[252], work[253]
	MOVUPD 2016(R9), X0
	MOVUPD 6112(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4032(R8)
	MOVUPD X3, 4048(R8)

	// (254,510) -> work[254], work[255]
	MOVUPD 4064(R9), X0
	MOVUPD 8160(R9), X1
	VADDPD X1, X0, X2
	VSUBPD X1, X0, X3
	MOVUPD X2, 4064(R8)
	MOVUPD X3, 4080(R8)
	INCQ BX
	CMPQ BX, $2
	JGE  size512_inv_128_r2_use_dst_stage1_done
	LEAQ 4096(R14), R8
	LEAQ 16(R15), R9
	JMP  size512_inv_128_r2_use_dst_stage1_pass

size512_inv_128_r2_use_dst_stage1_done:
	MOVQ R14, R8

size512_inv_128_r2_stage2:
	MOVQ $128, BX
	XORQ CX, CX

size512_inv_128_r2_stage2_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage3
	XORQ DX, DX

size512_inv_128_r2_stage2_j:
	CMPQ DX, $2
	JGE  size512_inv_128_r2_stage2_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $2, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage2_j

size512_inv_128_r2_stage2_next:
	ADDQ $4, CX
	JMP  size512_inv_128_r2_stage2_base

size512_inv_128_r2_stage3:
	MOVQ $64, BX
	XORQ CX, CX

size512_inv_128_r2_stage3_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage4
	XORQ DX, DX

size512_inv_128_r2_stage3_j:
	CMPQ DX, $4
	JGE  size512_inv_128_r2_stage3_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $4, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage3_j

size512_inv_128_r2_stage3_next:
	ADDQ $8, CX
	JMP  size512_inv_128_r2_stage3_base

size512_inv_128_r2_stage4:
	MOVQ $32, BX
	XORQ CX, CX

size512_inv_128_r2_stage4_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage5
	XORQ DX, DX

size512_inv_128_r2_stage4_j:
	CMPQ DX, $8
	JGE  size512_inv_128_r2_stage4_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $8, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage4_j

size512_inv_128_r2_stage4_next:
	ADDQ $16, CX
	JMP  size512_inv_128_r2_stage4_base

size512_inv_128_r2_stage5:
	MOVQ $16, BX
	XORQ CX, CX

size512_inv_128_r2_stage5_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage6
	XORQ DX, DX

size512_inv_128_r2_stage5_j:
	CMPQ DX, $16
	JGE  size512_inv_128_r2_stage5_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $16, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage5_j

size512_inv_128_r2_stage5_next:
	ADDQ $32, CX
	JMP  size512_inv_128_r2_stage5_base

size512_inv_128_r2_stage6:
	MOVQ $8, BX
	XORQ CX, CX

size512_inv_128_r2_stage6_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage7
	XORQ DX, DX

size512_inv_128_r2_stage6_j:
	CMPQ DX, $32
	JGE  size512_inv_128_r2_stage6_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $32, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage6_j

size512_inv_128_r2_stage6_next:
	ADDQ $64, CX
	JMP  size512_inv_128_r2_stage6_base

size512_inv_128_r2_stage7:
	MOVQ $4, BX
	XORQ CX, CX

size512_inv_128_r2_stage7_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage8
	XORQ DX, DX

size512_inv_128_r2_stage7_j:
	CMPQ DX, $64
	JGE  size512_inv_128_r2_stage7_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $64, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage7_j

size512_inv_128_r2_stage7_next:
	ADDQ $128, CX
	JMP  size512_inv_128_r2_stage7_base

size512_inv_128_r2_stage8:
	MOVQ $2, BX
	XORQ CX, CX

size512_inv_128_r2_stage8_base:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_stage9
	XORQ DX, DX

size512_inv_128_r2_stage8_j:
	CMPQ DX, $128
	JGE  size512_inv_128_r2_stage8_next

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $128, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	IMULQ BX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage8_j

size512_inv_128_r2_stage8_next:
	ADDQ $256, CX
	JMP  size512_inv_128_r2_stage8_base

size512_inv_128_r2_stage9:
	MOVQ $1, BX
	XORQ CX, CX
	XORQ DX, DX

size512_inv_128_r2_stage9_j:
	CMPQ DX, $256
	JGE  size512_inv_128_r2_scale

	MOVQ CX, SI
	ADDQ DX, SI
	SHLQ $4, SI
	MOVQ CX, DI
	ADDQ DX, DI
	ADDQ $256, DI
	SHLQ $4, DI

	MOVUPD (R8)(SI*1), X0
	MOVUPD (R8)(DI*1), X1

	MOVQ DX, AX
	SHLQ $4, AX
	MOVUPD (R10)(AX*1), X2

	VMOVDDUP X2, X3
	VPERMILPD $1, X2, X4
	VMOVDDUP X4, X4
	VPERMILPD $1, X1, X6
	VMULPD X4, X6, X6
	VFMSUBADD231PD X3, X1, X6

	VADDPD X6, X0, X7
	VSUBPD X6, X0, X8
	MOVUPD X7, (R8)(SI*1)
	MOVUPD X8, (R8)(DI*1)

	INCQ DX
	JMP  size512_inv_128_r2_stage9_j

size512_inv_128_r2_scale:
	// Apply 1/n scaling (1/512 = 0.001953125)
	MOVQ ·fiveHundredTwelfth64(SB), AX
	VMOVQ AX, X9
	VMOVDDUP X9, X9

	XORQ CX, CX
size512_inv_128_r2_scale_loop:
	CMPQ CX, $512
	JGE  size512_inv_128_r2_finalize
	MOVQ CX, SI
	SHLQ $4, SI
	MOVUPD (R8)(SI*1), X0
	VMULPD X9, X0, X0
	MOVUPD X0, (R8)(SI*1)
	INCQ CX
	JMP  size512_inv_128_r2_scale_loop

size512_inv_128_r2_finalize:
	MOVQ dst+0(FP), R9
	CMPQ R8, R9
	JE   size512_inv_128_r2_done

	XORQ CX, CX
size512_inv_128_r2_copy_loop:
	VMOVUPS (R8)(CX*1), Y0
	VMOVUPS Y0, (R9)(CX*1)
	ADDQ $32, CX
	CMPQ CX, $8192
	JL   size512_inv_128_r2_copy_loop

size512_inv_128_r2_done:
	VZEROUPPER
	MOVB $1, ret+96(FP)
	RET

size512_inv_128_r2_return_false:
	VZEROUPPER
	MOVB $0, ret+96(FP)
	RET
